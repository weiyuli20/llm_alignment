{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cc21bafa",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0bb6edff",
   "metadata": {},
   "source": [
    "强化学习是不同于监督学习和无监督学习的第三种范式，它试图模拟人类与环境交互的学习过程\n",
    "\n",
    "强化学习是拟人，深度学习是模拟大脑神经元\n",
    "\n",
    "智能体与环境交互中的试错式学习，在奖惩机制中学习，分幕任务中在稀疏奖励和延迟奖励鼓舞下学习，在最优策略和远大目标驱动下学习\n",
    "\n",
    "试错与延迟奖励是强化学习两个最显著特征\n",
    "\n",
    "动作往往影响的不是即时奖励，也会影响下一个状态，从而影响后续的奖励。 连锁反应\n",
    "\n",
    "闭环学习，智能体与环境连续不断地闭环交互"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef621ed3",
   "metadata": {},
   "source": [
    "随机变量的取值是定义在随机实验的样本空间里的  随机变量具有随机性\n",
    "\n",
    "随机实验的一次结果 ->观测值 没有随机性\n",
    "\n",
    "随机抽样：一个箱子里有红球，黑球\n",
    "\n",
    "概率密度函数， 每个随机事件的发生概率\n",
    "\n",
    "期望是客观存在的。\n",
    "如果概率分布是已知的，期望是确定的，例如抛硬币，掷骰子\n",
    "\n",
    "如果概率分布是未知的：进行大量的随机实验，观测值取平均\n",
    "\n",
    "\n",
    "期望的一个重要用途：通过求期望可以消除公式中的一些随机变量\n",
    "\n",
    "对哪个随机变量求期望，就可以消掉哪个随机变量"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54145d21",
   "metadata": {},
   "source": [
    "智能体-环境交互\n",
    "\n",
    "（1）马尔科夫决策过程\n",
    "\n",
    "马尔可夫性是指系统的下一状态仅与当前状态有关。\n",
    "\n",
    "$ Pr[S_{t+1}|S_t] = Pr[S_{t+1}|S_1,S_2,...,S_t] $\n",
    "\n",
    "直观理解为$S_t$已经包含了过往的信息\n",
    "\n",
    "概念：\n",
    "\n",
    "状态\n",
    "\n",
    "状态是做决策的依据，动作可以影响环境从而改变状态\n",
    "\n",
    "\n",
    "状态空间\n",
    "\n",
    "动作  智能体唯一能决定的就是根据策略做动作\n",
    "\n",
    "动作空间\n",
    "\n",
    "奖励\n",
    "\n",
    "奖励是强化学习设计者根据智能体的终极目标提前定义的量化数值，强化学习效果的好坏与奖励的定义方法有直接关系。\n",
    "\n",
    "状态有限，动作有限，奖励明确 ---强化学习中的一种\n",
    "\n",
    "\n",
    "S0, A0, R1, S1, A1, R2, S2, A2, R3, S3 ...\n",
    "\n",
    "$p(s',r| s,a) = Pr(S_t=s',R_t = r| S_{t-1} =s, A_{t-1} = a)$\n",
    "\n",
    "\n",
    "$\\sum_{s' \\in  S}^{}  \\sum_{r \\in R}^{} p(s',r| s,a) = 1 $\n",
    "\n",
    "\n",
    "状态转移概率：\n",
    "\n",
    "$p(s'| s,a) = Pr(S_t=s'| S_{t-1} =s, A_{t-1} = a) = \\sum_{r\\in R}^{} p(s',r| s,a)$\n",
    "\n",
    "奖励概率：\n",
    "\n",
    "$p(r| s,a) = Pr(R_t=r | S_{t-1} =s, A_{t-1} = a) = \\sum_{s'\\in S}^{} p(s',r| s,a)$\n",
    "\n",
    "假设任务中奖励只能是−1（惩罚）、0（无奖励）、1（正奖励），那么 “所有可能的r” 就是这三个值。当计算 “转移到s′且获得任意奖励时，需要把 “转移到s′且得−1，转移到s′且得0，转移到s′且得1这三种情况的概率全部相加，最终得到转移到s′的总概率。"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
