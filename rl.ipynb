{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0bb6edff",
   "metadata": {},
   "source": [
    "强化学习是不同于监督学习和无监督学习的第三种范式，它试图模拟人类与环境交互的学习过程\n",
    "\n",
    "强化学习是拟人，深度学习是模拟大脑神经元\n",
    "\n",
    "智能体与环境交互中的试错式学习，在奖惩机制中学习，分幕任务中在稀疏奖励和延迟奖励鼓舞下学习，在最优策略和远大目标驱动下学习\n",
    "\n",
    "试错与延迟奖励是强化学习两个最显著特征\n",
    "\n",
    "动作往往影响的不是即时奖励，也会影响下一个状态，从而影响后续的奖励。 连锁反应\n",
    "\n",
    "闭环学习，智能体与环境连续不断地闭环交互"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef621ed3",
   "metadata": {},
   "source": [
    "随机变量的取值是定义在随机实验的样本空间里的  随机变量具有随机性\n",
    "\n",
    "随机实验的一次结果 ->观测值 没有随机性\n",
    "\n",
    "随机抽样：一个箱子里有红球，黑球\n",
    "\n",
    "概率密度函数， 每个随机事件的发生概率\n",
    "\n",
    "期望是客观存在的。\n",
    "如果概率分布是已知的，期望是确定的，例如抛硬币，掷骰子\n",
    "\n",
    "如果概率分布是未知的：进行大量的随机实验，观测值取平均\n",
    "\n",
    "\n",
    "期望的一个重要用途：通过求期望可以消除公式中的一些随机变量\n",
    "\n",
    "对哪个随机变量求期望，就可以消掉哪个随机变量"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54145d21",
   "metadata": {},
   "source": [
    "智能体-环境交互\n",
    "\n",
    "（1）马尔科夫决策过程\n",
    "\n",
    "马尔可夫性是指系统的下一状态仅与当前状态有关。\n",
    "\n",
    "$ Pr[S_{t+1}|S_t] = Pr[S_{t+1}|S_1,S_2,...,S_t] $\n",
    "\n",
    "直观理解为$S_t$已经包含了过往的信息\n",
    "\n",
    "概念：\n",
    "\n",
    "状态\n",
    "\n",
    "状态是做决策的依据，动作可以影响环境从而改变状态\n",
    "\n",
    "\n",
    "状态空间\n",
    "\n",
    "动作  智能体唯一能决定的就是根据策略做动作\n",
    "\n",
    "动作空间\n",
    "\n",
    "奖励\n",
    "\n",
    "奖励是强化学习设计者根据智能体的终极目标提前定义的量化数值，强化学习效果的好坏与奖励的定义方法有直接关系。\n",
    "\n",
    "状态有限，动作有限，奖励明确--- 强化学习中的一种\n",
    "\n",
    "动态特性函数:\n",
    "\n",
    "S0, A0, R1, S1, A1, R2, S2, A2, R3, S3 ...\n",
    "\n",
    "$p(s',r| s,a) = Pr(S_t=s',R_t = r| S_{t-1} =s, A_{t-1} = a)$\n",
    "\n",
    "$\\sum_{s' \\in  S}^{}  \\sum_{r \\in R}^{} p(s',r| s,a) = 1 $\n",
    "\n",
    "状态转移函数：\n",
    "\n",
    "$p(s'| s,a) = Pr(S_t=s'| S_{t-1} =s, A_{t-1} = a) = \\sum_{r\\in R}^{} p(s',r| s,a)$\n",
    "\n",
    "奖励函数：\n",
    "\n",
    "$p(r| s,a) = Pr(R_t=r | S_{t-1} =s, A_{t-1} = a) = \\sum_{s'\\in S}^{} p(s',r| s,a)$\n",
    "\n",
    "双参数奖励函数：\n",
    "\n",
    "$r(s,a) = E[R_t|S_{t-1} = s, A_{t-1} = a] = \\sum_{r\\in R}^{} r\\sum_{s' \\in  S}^{} p(s',r|s,a) = \\sum_{r\\in R}^{} r * p(r| s,a)$\n",
    "\n",
    "\n",
    "强化学习的**目标**就是追求累积奖励最大化，也就是**实现最大化的回报**，如何定义回报是强化学习首先应该解决的问题\n",
    "\n",
    "**回报**$G_t$是奖励的总和.\n",
    "\n",
    "$G_t = R_{t+1} + R_{t+2} + R_{t+3} +.. + R_{T}$\n",
    "\n",
    "$G_t$可能会趋于无穷大，$G_t$不收敛；不能衡量智能体的远见卓识与目标短浅，难以处理未来奖励与现在奖励的权重关系；不能防止智能体做无用功。\n",
    "\n",
    "改进公式：\n",
    "\n",
    "$G_t  =R_{t+1} + \\gamma R_{t+2} + \\gamma ^2 R_{t+3} +...  = \\sum_{k=0}^{\\infty} \\gamma^k R_{t+k+1}$\n",
    "\n",
    "$0 =< \\gamma <=1$,称为折扣率\n",
    "\n",
    "$ \\gamma $小，更看重当前奖励，容易目光短浅，$\\gamma$大，目光长远\n",
    "\n",
    "从拟人的角度看，一个具有远见卓识的人，会追求长远收获，而不仅仅是追求眼前利益；对于一个旗手来说，一定为赢棋这种终极大奖为目标，不会追求吃子这种即时奖励\n",
    "\n",
    "$G_t$是一个随机变量\n",
    "\n",
    "$G_t  =R_{t+1} + \\gamma G_{t+1} = R_{t+1} + \\gamma R_{t+2} + \\gamma ^2 G_{t+2}$\n",
    "\n",
    "\n",
    "**策略和价值函数**\n",
    "\n",
    "策略函数： \n",
    "\n",
    "$\\pi(a|s) = Pr_{\\pi}(A =a | S  =s)$\n",
    "\n",
    "\n",
    "强化学习的任务就是学习到最优策略\n",
    "\n",
    "智能体执行动作是随机抽样的结果\n",
    "\n",
    "强化学习的目标虽然是回报最大化，但是回报公式中都是随机变量，随机变量不存在所谓的最大值，即使存在也没有什么价值。**追求随机变量\n",
    "\n",
    "的最大化，就是期望的最大化**。由此产生了状态价值函数和动作价值函数两个概念。\n",
    "\n",
    "$v_{\\pi}(s)$ 既是状态的函数，也是策略的函数，在某种策略下，$v_{\\pi}(s)$的值越大，说明状态越好，同理在某状态下，$v_{\\pi}(s)$的1值越大，说明策略越好\n",
    "\n",
    "$v_{\\pi}(s) = E_{\\pi}[G_t|S_t  = s] = E_{\\pi}[R_{t+1}+ \\gamma G_{t+1}|S_t  = s]$,策略越好，状态价值函数$v_{\\pi}(s)$的值越大\n",
    "\n",
    "$q_{\\pi}(s,a) = E_{\\pi}[G_t|S_t  = s, A_t  = a] = E_{\\pi}[R_{t+1}+ \\gamma G_{t+1}|S_t  = s,A_t = a]$,$q_{\\pi}(s,a) $称为策略$\\pi$的动作价值函数，即策略越好，动作价值函数的值越大\n",
    "\n",
    "\n",
    "$v_{\\pi}(s) $和 $q_{\\pi}(s,a)$ 称为初始贝尔曼方程，两者就差一个动作\n",
    "\n",
    "当动作是唯一确定的，$v_{\\pi}(s) $ = $q_{\\pi}(s,a)$\n",
    "\n",
    "**最优策略与最优价值函数**\n",
    "\n",
    "$\\pi_{\\star}(a|s)$  强化学习终极目标\n",
    "\n",
    "$q_{\\star}(s,a) =\\max_{\\pi}  q_{\\pi}(s,a)  $ 强化学习间接目标\n",
    "\n",
    "$v_{\\star}(s) = \\max_{\\pi}  v_{\\pi}(s)$ 强化学习间接目标\n",
    "\n",
    "显然，最优状态价值函数只和状态有关，而与策略$\\pi$无关，同样，最优动作价值函数只与状态-动作对有关，与策略也没有关系\n",
    "\n",
    "$q_{\\star}$ 非常有用，它具有高超的洞察力，一眼能看出当前状态执行哪个动作能获得最优终极目标\n",
    "\n",
    "\n",
    "**贝尔曼期望方程**\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
