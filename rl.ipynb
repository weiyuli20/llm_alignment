{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cc21bafa",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0bb6edff",
   "metadata": {},
   "source": [
    "强化学习是不同于监督学习和无监督学习的第三种范式，它试图模拟人类与环境交互的学习过程\n",
    "\n",
    "强化学习是拟人，深度学习是模拟大脑神经元\n",
    "\n",
    "智能体与环境交互中的试错式学习，在奖惩机制中学习，分幕任务中在稀疏奖励和延迟奖励鼓舞下学习，在最优策略和远大目标驱动下学习\n",
    "\n",
    "试错与延迟奖励是强化学习两个最显著特征\n",
    "\n",
    "动作往往影响的不是即时奖励，也会影响下一个状态，从而影响后续的奖励。 连锁反应\n",
    "\n",
    "闭环学习，智能体与环境连续不断地闭环交互"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef621ed3",
   "metadata": {},
   "source": [
    "随机变量的取值是定义在随机实验的样本空间里的  随机变量具有随机性\n",
    "\n",
    "随机实验的一次结果 ->观测值 没有随机性\n",
    "\n",
    "随机抽样：一个箱子里有红球，黑球\n",
    "\n",
    "概率密度函数， 每个随机事件的发生概率\n",
    "\n",
    "期望是客观存在的。\n",
    "如果概率分布是已知的，期望是确定的，例如抛硬币，掷骰子\n",
    "\n",
    "如果概率分布是未知的：进行大量的随机实验，观测值取平均\n",
    "\n",
    "\n",
    "期望的一个重要用途：通过求期望可以消除公式中的一些随机变量\n",
    "\n",
    "对哪个随机变量求期望，就可以消掉哪个随机变量"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54145d21",
   "metadata": {},
   "source": [
    "智能体-环境交互\n",
    "\n",
    "（1）马尔科夫决策过程\n",
    "\n",
    "马尔可夫性是指系统的下一状态仅与当前状态有关。\n",
    "\n",
    "$ Pr[S_{t+1}|S_t] = Pr[S_{t+1}|S_1,S_2,...,S_t] $\n",
    "\n",
    "直观理解为$S_t$已经包含了过往的信息\n",
    "\n",
    "概念：\n",
    "\n",
    "状态\n",
    "\n",
    "状态是做决策的依据，动作可以影响环境从而改变状态\n",
    "\n",
    "\n",
    "状态空间\n",
    "\n",
    "动作  智能体唯一能决定的就是根据策略做动作\n",
    "\n",
    "动作空间\n",
    "\n",
    "奖励\n",
    "\n",
    "奖励是强化学习设计者根据智能体的终极目标提前定义的量化数值，强化学习效果的好坏与奖励的定义方法有直接关系。\n",
    "\n",
    "状态有限，动作有限，奖励明确 ---强化学习中的一种\n",
    "\n",
    "\n",
    "S0, A0, R1, S1, A1, R2, S2, A2, R3, S3 ...\n",
    "\n",
    "$p(s',r| s,a) = Pr(S_t=s',R_t = r| S_{t-1} =s, A_{t-1} = a)$\n",
    "\n",
    "\n",
    "$\\sum_{s' \\in  S}^{}  \\sum_{r \\in R}^{} p(s',r| s,a) = 1 $\n",
    "\n",
    "\n",
    "状态转移概率：\n",
    "\n",
    "$p(s'| s,a) = Pr(S_t=s'| S_{t-1} =s, A_{t-1} = a) = \\sum_{r\\in R}^{} p(s',r| s,a)$\n",
    "\n",
    "奖励概率：\n",
    "\n",
    "$p(r| s,a) = Pr(R_t=r | S_{t-1} =s, A_{t-1} = a) = \\sum_{s'\\in S}^{} p(s',r| s,a)$\n",
    "\n",
    "假设任务中奖励只能是−1（惩罚）、0（无奖励）、1（正奖励），那么 “所有可能的r” 就是这三个值。当计算 “转移到s′且获得任意奖励时，需要把 “转移到s′且得−1，转移到s′且得0，转移到s′且得1这三种情况的概率全部相加，最终得到转移到s′的总概率。\n",
    "\n",
    "\n",
    "双参数奖励函数：\n",
    "\n",
    "$r(s,a) = E[R_t|S_{t-1} = s, A_{t-1} = a] = \\sum_{r\\in R}^{} r\\sum_{s' \\in  S}^{} p(s',r|s,a) = \\sum_{r\\in R}^{} r * p(r| s,a)$\n",
    "\n",
    "\n",
    "强化学习的**目标**就是追求累积奖励最大化，也就是**实现最大化的回报**，如何定义回报是强化学习首先应该解决的问题\n",
    "\n",
    "**回报**$G_t$是奖励的总和.\n",
    "\n",
    "$G_t = R_{t+1} + R_{t+2} + R_{t+3} +.. + R_{T}$\n",
    "\n",
    "$G_t$可能会趋于无穷大，$G_t$不收敛；不能衡量智能体的远见卓识与目标短浅，难以处理未来奖励与现在奖励的权重关系；不能防止智能体做无用功。\n",
    "\n",
    "改进公式：\n",
    "\n",
    "$G_t  =R_{t+1} + \\gamma R_{t+2} + \\gamma ^2 R_{t+3} +...  = \\sum_{k=0}^{\\infty} \\gamma^k R_{t+k+1}$\n",
    "\n",
    "$0 =< \\gamma <=1$,称为折扣率\n",
    "\n",
    "$ \\gamma $小，更看重当前奖励，容易目光短浅，$\\gamma$大，目光长远\n",
    "\n",
    "从拟人的角度看，一个具有远见卓识的人，会追求长远收获，而不仅仅是追求眼前利益；对于一个旗手来说，一定为赢棋这种终极大奖为目标，不会追求吃子这种即时奖励\n",
    "\n",
    "$G_t$是一个随机变量\n",
    "\n",
    "$G_t  =R_{t+1} + \\gamma G_{t+1} = R_{t+1} + \\gamma R_{t+2} + \\gamma ^2 G_{t+2}$\n",
    "\n",
    "\n",
    "**策略和价值函数**\n",
    "\n",
    "策略函数： \n",
    "\n",
    "$\\pi(a|s) = Pr_{\\pi}(A =a | S  =s)$\n",
    "\n",
    "\n",
    "强化学习的任务就是学习到最优策略\n",
    "\n",
    "智能体执行动作是随机抽样的结果\n",
    "\n",
    "强化学习的目标虽然是回报最大化，但是回报公式中都是随机变量，随机变量不存在所谓的最大值，即使存在也没有什么价值。**追求随机变量\n",
    "\n",
    "的最大化，就是期望的最大化**。由此产生了状态价值函数和动作价值函数两个概念。\n",
    "\n",
    "$v_{\\pi}(s)$ 既是状态的函数，也是策略的函数，在某种策略下，$v_{\\pi}(s)$的值越大，说明状态越好，同理在某状态下，$v_{\\pi}(s)$的1值越大，说明策略越好\n",
    "\n",
    "$v_{\\pi}(s) = E_{\\pi}[G_t|S_t  = s] = E_{\\pi}[R_{t+1}+ \\gamma G_{t+1}|S_t  = s]$,策略越好，状态价值函数$v_{\\pi}(s)$的值越大\n",
    "\n",
    "$q_{\\pi}(s,a) = E_{\\pi}[G_t|S_t  = s, A_t  = a] = E_{\\pi}[R_{t+1}+ \\gamma G_{t+1}|S_t  = s,A_t = a]$,$q_{\\pi}(s,a) $称为策略$\\pi$的动作价值函数，即策略越好，动作价值函数的值越大\n",
    "\n",
    "\n",
    "$v_{\\pi}(s) $和 $q_{\\pi}(s,a)$ 称为初始贝尔曼方程，两者就差一个动作\n",
    "\n",
    "当动作是唯一确定的，$v_{\\pi}(s) $ = $q_{\\pi}(s,a)$\n",
    "\n",
    "**最优策略与最优价值函数**\n",
    "\n",
    "$\\pi_{\\star}(a|s)$  强化学习终极目标\n",
    "\n",
    "$q_{\\star}(s,a) =\\max_{\\pi}  q_{\\pi}(s,a)  $ 强化学习间接目标\n",
    "\n",
    "$v_{\\star}(s) = \\max_{\\pi}  v_{\\pi}(s)$ 强化学习间接目标\n",
    "\n",
    "显然，最优状态价值函数只和状态有关，而与策略$\\pi$无关，同样，最优动作价值函数只与状态-动作对有关，与策略也没有关系\n",
    "\n",
    "$q_{\\star}$ 非常有用，它具有高超的洞察力，一眼能看出当前状态执行哪个动作能获得最优终极目标\n",
    "\n",
    "\n",
    "**贝尔曼期望方程**\n",
    "$v_{\\pi}(s)  =E_{\\pi}[R_{t+1} + \\gamma G_{t+1} |S_t  =s]$\n",
    "\n",
    "$ = E_{\\pi}[R_{t+1} + \\gamma v_{\\pi}(S')  |S_t  =s]$\n",
    "\n",
    "$ = \\sum_{a}^{} \\pi(a|s)\\sum_{s',r}^{}p(s',r|s,a)[r+\\gamma v_{\\pi}(s')] $\n",
    "\n",
    "强化学习两种学习方式:\n",
    "\n",
    "基于值：学习价值函数\n",
    "\n",
    "value-based的方法里，是没有policy的，policy是人为指定。\n",
    "\n",
    "例如，在给定价值函数时，如果我们希望智能体每次采取的行动都能获得最大奖励，这是greedy policy\n",
    "\n",
    "基于策略：直接学习策略\n",
    "\n",
    "强化学习的最终目标是找到最优策略\n",
    "\n",
    "policy_based的方法通过直接train policy得到最佳策略\n",
    "\n",
    "而value_based的方法通过找到最优价值函数间接确定最佳策略。\n",
    "\n",
    "事实上，大多数时候，value-based的方法会使用epsilon-greedy策略平衡探索和利用。\n",
    "\n",
    "value_based的方法有两种计算方式：state value 、action value\n",
    "\n",
    "贝尔曼方程简化了state-value和action-value的计算\n",
    "\n",
    "\n",
    "学习value function和 policy function的两种学习策略：蒙特卡洛和时序差分\n",
    "\n",
    "epsilon-greedy是一种在探索和利用之间交替进行的策略\n",
    "\n",
    "\n",
    "value_based 方法最终得到的策略是确定性策略\n",
    "\n",
    "\n",
    "reinforce\n",
    "\n",
    "用蒙特卡洛采样计算return 是无偏的，但是由于采样数量少，每次采样只有10、100 几百条轨迹，因此不同时刻采样的数据计算出的return有很大的方差。\n",
    "\n",
    "高方差会导致收敛速度变慢，训练困难。  消除高方差的方法是增加一个batch内采样轨迹数量，但是由于在这个方法中，样本使用一次就丢弃了，增加样本数会reduce sample efficiency.\n",
    "\n",
    "价值的变化传递给策略的桥梁\n",
    "\n",
    "\n",
    "降低方差的方式 actor-critic  使用TD error 或者 advantage function 替换MC return,但是需要引入critic network估计状态价值。\n",
    "\n",
    "MC REINFORCE 是 “轨迹级更新”（需等轨迹结束后一次性更新），而 Actor-Critic 是 “单步更新”（每一步交互后即可更新）\n",
    "\n",
    "\n",
    "\n",
    "RLHF:\n",
    "涉及四个模型：Actor, Critic, RewardModel、reference model\n",
    "\n",
    "actor critic 是要进行训练的\n",
    "\n",
    "reward 和 reference model参数是冻结的\n",
    "\n",
    "\n",
    "actor是 sft model\n",
    "\n",
    "reference model 一般也是sft model,  actor 和 reference 之间使用KL散度进行约束，防止actor 跑偏\n",
    "\n",
    "critic reward 使用的是训练好的奖励模型。\n",
    "\n",
    "---------------------------------------------------------------------------------------------------\n",
    "\n",
    "rlhf 的第一步，生成经验数据： 输出batch prompt ,由actor生成完整的响seq. seq = prompt + response.\n",
    "\n",
    "然后将seq输出到 actor 网络，输出response 每个token对应的logits\n",
    "\n",
    "将seq输入到reference model, 输出response 每个token对应的logits\n",
    "\n",
    "将seq 输入·到reward model ,对整个序列进行打分，而不是对每个token进行打分\n",
    "\n",
    "将seq 输入到critic model,对response里每个token计算状态价值\n",
    "\n",
    "```\n",
    "    def generate_experience(self, prompts):\n",
    "        self.eval() # 开启eval模式\n",
    "        # 输入instruct prompt，由Actor生成seq，上图中红色步骤（1），seq由instruct和response组成\n",
    "        seq = self._generate_sequence(prompts)\n",
    "        self.train() # 恢复训练模型\n",
    "        pad_token_id = self.tokenizer.pad_token_id\n",
    "        attention_mask = seq.not_equal(pad_token_id).long()\n",
    "        with torch.no_grad():\n",
    "            # 将seq喂入actor中得到action_logits，上图中棕色步骤（2）\n",
    "            output = self.actor_model(seq, attention_mask=attention_mask)\n",
    "            # 将seq喂入SFT中得到sft_logits，上图中黑色步骤（5）\n",
    "            output_ref = self.ref_model(seq, attention_mask=attention_mask)\n",
    "            # 将seq喂入reward模型中打分，得到r(x,  y)，上图绿色步骤（4）\n",
    "            reward_score = self.reward_model.forward_value(\n",
    "                seq, attention_mask,\n",
    "                prompt_length=self.prompt_length)['chosen_end_scores'].detach(\n",
    "                )\n",
    "            # 将seq喂入critic，获得critic的value，上图蓝色步骤（3）\n",
    "            values = self.critic_model.forward_value(\n",
    "                seq, attention_mask, return_value_only=True).detach()[:, :-1]\n",
    "\n",
    "        logits = output.logits\n",
    "        logits_ref = output_ref.logits\n",
    "        # 获得经验数据，存入经验池\n",
    "        return {\n",
    "            'prompts': prompts,\n",
    "            'logprobs': gather_log_probs(logits[:, :-1, :], seq[:, 1:]),\n",
    "            'ref_logprobs': gather_log_probs(logits_ref[:, :-1, :], seq[:, 1:]),\n",
    "            'value': values,\n",
    "            'rewards': reward_score,\n",
    "            'input_ids': seq,\n",
    "            \"attention_mask\": attention_mask\n",
    "        }\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "计算 actor logits 和 reference  logits 之间的KL散度   ：对应位置的目标token对应的log logits 相减\n",
    "\n",
    "奖励计算方式：终端奖励\n",
    "\n",
    "    reward 模型输出的是序列级别的奖励，这个奖励只加在最后一个token上。\n",
    "\n",
    "    实际奖励的计算方式不止reward模型输出的奖励，需要将token级别的kl散度和reward相加得到最终的奖励。\n",
    "\n",
    "\n",
    "    计算advantage\n",
    "\n",
    "\n",
    "    根据经验数据和advantage 计算actor loss, 进行反向传播和参数更新\n",
    "\n",
    "// https://blog.csdn.net/qq_36426650/article/details/130814286\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
