{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "10df4825",
   "metadata": {},
   "source": [
    "# 动手实现LLaMA2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70625669",
   "metadata": {},
   "source": [
    "## 1. 超参数定义\n",
    "自定义一个ModelConfig类，继承自transformers库中的PretrainedConfig类，用来存储和记录超参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "18fcf68d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import PretrainedConfig\n",
    "\n",
    "class ModelConfig(PretrainedConfig):\n",
    "    model_type=\"Tiny-K\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim: int =768, #模型维度\n",
    "        n_layers: int = 12, #transformer 层数\n",
    "        n_heads: int = 16,  #注意力头数\n",
    "        n_kv_heads: int = 8,\n",
    "        vocab_size: int =6144, #词汇表大小\n",
    "        hidden_dim: int = None,\n",
    "        multiple_of: int = 64,\n",
    "        norms_eps: float = 1e-5,\n",
    "        max_seq_len: int=512, #输入的最大序列长度\n",
    "        dropout: float=0.0,\n",
    "        flash_atten: bool =True,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        self.dim = dim\n",
    "        self.n_layers = n_layers\n",
    "        self.n_heads = n_heads\n",
    "        self.n_kv_heads = n_kv_heads\n",
    "        self.vocab_size = vocab_size\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.multiple_of = multiple_of\n",
    "        self.norms_eps = norms_eps\n",
    "        self.dropout = dropout\n",
    "        self.flash_atten = flash_atten\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "\n",
    "args = ModelConfig()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "683fe01b",
   "metadata": {},
   "source": [
    "## RMSNorm\n",
    "\n",
    "\n",
    "$\\operatorname{RMSNorm}(x)=\\frac{x}{\\sqrt{\\frac{1}{n} \\sum_{i=1}^{n} x_{i}^{2}+\\epsilon}} \\cdot \\gamma $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "02b70e3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 50, 768])\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "class RMSNorm(nn.Module):\n",
    "    def __init__(self, dim: int, eps: float):\n",
    "        super().__init__()\n",
    "\n",
    "        self.eps = eps\n",
    "        self.weight = nn.Parameter(torch.ones(dim))\n",
    "\n",
    "    def _norm(self,x):\n",
    "        return x * torch.rsqrt(x.pow(2).mean(-1,keepdim=True) + self.eps)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        output = self._norm(x.float()).type_as(x)\n",
    "        return output * self.weight\n",
    "\n",
    "norm  =RMSNorm(args.dim, args.norms_eps)\n",
    "x = torch.randn(1,50,args.dim)\n",
    "output =norm(x)\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92dc9149",
   "metadata": {},
   "source": [
    "## LLaMA2 Attention\n",
    "在LLaMA模型中，虽然只有LLaMA-70B模型使用了分组注意力机制（GQA),但我们此处依然使用GQA来构建LLaMA Attention 模块，以节省显存占用，提升模型效率"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46cf1916",
   "metadata": {},
   "source": [
    "### 1.repeat_kv\n",
    "将键和值扩展到和查询的维度一样"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7f7f6a22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def repeat_kv(x:torch.Tensor, n_rep: int) -> torch.Tensor:\n",
    "    #获取输入张量的形状：批量大小，序列长度、键、值对头的数量\n",
    "    bs,slen,n_kv_heads, head_dim = x.shape\n",
    "\n",
    "    #如果重复次数为1，则不需要重复，直接返回原始张量\n",
    "    if n_rep ==1:\n",
    "        return x\n",
    "    \n",
    "    #对张量进行扩展和重塑操作以重复键值对\n",
    "    return (\n",
    "        x[:,:,:,None,:]\n",
    "        .expands(bs,slen,n_kv_heads,n_rep,head_dim)\n",
    "        .reshape(bs,slen, n_kv_heads * n_rep, head_dim)\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89602558",
   "metadata": {},
   "source": [
    "### 2. 旋转嵌入\n",
    "旋转嵌入可以为注意力机制提供更强的上下文"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "75aa4db2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def precompute_freqs_cis(dim: int, end:int, theta: float=10000.0):\n",
    "\n",
    "    #生成频率序列\n",
    "    freqs = 1.0 / (theta ** (torch.arange(0,dim, 2)[:(dim //2)].float() /dim))\n",
    "    #生成时间序列\n",
    "    t = torch.arange(end, device=freqs.device)\n",
    "    #计算频率的外积\n",
    "    freqs = torch.outer(t,freqs).float()\n",
    "    #计算频率的余弦值，得到实部\n",
    "    freqs_cos = torch.cos(freqs)\n",
    "    #计算频率的正弦值，得到虚部\n",
    "    freqs_sin =torch.sin(freqs)\n",
    "\n",
    "    return freqs_cos,freqs_sin\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6f6a97cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reshape_for_broadcast(freqs_cis:torch.Tensor, x:torch.Tensor):\n",
    "    '''\n",
    "    调整张量freqs_cls的形状，使其在进行广播操作时与x的维度对齐\n",
    "    '''\n",
    "    #获取x的维度\n",
    "    ndim = x.ndim\n",
    "\n",
    "    assert 0<=1<ndim\n",
    "\n",
    "    #确保freqs_cis的形状与x的第二维和最后一维形同\n",
    "    assert freqs_cis.shape == (x.shape[1],x.shape[-1])\n",
    "\n",
    "    #构造一个新的形状，除了第二维和最后一维，其他维度都为1，这样做是为了能够将freqs_cis与x进行广播对齐\n",
    "    shape = [d if i==1 or i ==ndim -1 else 1 for i,d in enumerate(x.shape)]\n",
    "    \n",
    "    #将freqs_cis调整为新的形状，并返回\n",
    "    return freqs_cis.view(shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "924bc82d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "\n",
    "def apply_rotary_emb(\n",
    "        xq: torch.Tensor,\n",
    "        xk: torch.Tensor,\n",
    "        freqs_cos: torch.Tensor,\n",
    "        freqs_sin: torch.Tensor\n",
    ")->Tuple[torch.Tensor,torch.Tensor]:\n",
    "    '''实现旋转嵌入'''\n",
    "\n",
    "    #将查询和键张量转换为浮点数，并重塑形状以分离实部和虚部\n",
    "    xq_r,xq_i = xq.float().reshape(xq.shape[:-1] + (-1,2)).unbind(-1)\n",
    "    xk_r,xk_i = xk.float().reshape(xk.shape[:-1] + (-1,2)).unbind(-1)\n",
    "\n",
    "    #重新塑性频率张量已进行广播\n",
    "    freqs_cos = reshape_for_broadcast(freqs_cos,xq_r)\n",
    "    freqs_sin = reshape_for_broadcast(freqs_sin,xq_r)\n",
    "\n",
    "    #应用旋转，分别计算旋转后的实部和虚部\n",
    "    xq_out_r  =xq_r * freqs_cos - xq_i * freqs_sin\n",
    "    xq_out_i = xq_r * freqs_sin + xq_i * freqs_cos\n",
    "    xk_out_r = xk_r * freqs_cos - xk_i * freqs_sin\n",
    "    xk_out_i = xk_r * freqs_sin + xk_i * freqs_cos\n",
    "\n",
    "    #将最后两个维度合并，并还原为原始张量的形状\n",
    "    xq_out = torch.stack([xq_out_r,xq_out_i],dim = -1).flatten(3)\n",
    "    xk_out = torch.stack([xk_out_r,xk_out_i],dim = -1).flatten(3)\n",
    "\n",
    "    return xq_out.type_as(xq),xk_out.type_as(xk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2912cf4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([50, 24]) torch.Size([50, 24])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 50, 6, 48]), torch.Size([1, 50, 6, 48]))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xq = torch.randn(1,50,6,48) # bs, seq_len, dim//n_head, n_head_dim\n",
    "xk = torch.randn(1,50,6,48)\n",
    "\n",
    "cos,sin = precompute_freqs_cis(288//6,50)\n",
    "print(cos.shape, sin.shape)\n",
    "xq_out,xk_out  =apply_rotary_emb(xq,xk,cos,sin)\n",
    "\n",
    "xq_out.shape,xk_out.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "410d36c2",
   "metadata": {},
   "source": [
    "### 组装LLaMA2 Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0b6b544",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Tensor' object has no attribute 'expands'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[1;32mIn [24]\u001b[0m, in \u001b[0;36m<cell line: 106>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    103\u001b[0m x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrand(batch_size,seq_len,dim) \u001b[38;5;66;03m#随机生成输入张量\u001b[39;00m\n\u001b[0;32m    105\u001b[0m freqs_cos,freqs_sin  \u001b[38;5;241m=\u001b[39mprecompute_freqs_cis(dim \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m args\u001b[38;5;241m.\u001b[39mn_heads, seq_len)\n\u001b[1;32m--> 106\u001b[0m output  \u001b[38;5;241m=\u001b[39m\u001b[43mattention_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfreqs_cos\u001b[49m\u001b[43m,\u001b[49m\u001b[43mfreqs_sin\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    108\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOutput shape:\u001b[39m\u001b[38;5;124m\"\u001b[39m,output\u001b[38;5;241m.\u001b[39mshape)\n",
      "File \u001b[1;32md:\\software\\pythontools\\anaconda3\\envs\\RetinaNet\\lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\software\\pythontools\\anaconda3\\envs\\RetinaNet\\lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Input \u001b[1;32mIn [24]\u001b[0m, in \u001b[0;36mAttention.forward\u001b[1;34m(self, x, freqs_cos, freqs_sin)\u001b[0m\n\u001b[0;32m     64\u001b[0m xq,xk  \u001b[38;5;241m=\u001b[39mapply_rotary_emb(xq,xk,freqs_cos,freqs_sin)\n\u001b[0;32m     66\u001b[0m \u001b[38;5;66;03m#对键和值进行扩展以适应重复次数\u001b[39;00m\n\u001b[1;32m---> 67\u001b[0m xk  \u001b[38;5;241m=\u001b[39m\u001b[43mrepeat_kv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mxk\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_rep\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     68\u001b[0m xv  \u001b[38;5;241m=\u001b[39mrepeat_kv(xv,\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_rep)\n\u001b[0;32m     70\u001b[0m xq \u001b[38;5;241m=\u001b[39m xq\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m2\u001b[39m)\n",
      "Input \u001b[1;32mIn [7]\u001b[0m, in \u001b[0;36mrepeat_kv\u001b[1;34m(x, n_rep)\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m#对张量进行扩展和重塑操作以重复键值对\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[1;32m---> 11\u001b[0m     \u001b[43mx\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexpands\u001b[49m(bs,slen,n_kv_heads,n_rep,head_dim)\n\u001b[0;32m     13\u001b[0m     \u001b[38;5;241m.\u001b[39mreshape(bs,slen, n_kv_heads \u001b[38;5;241m*\u001b[39m n_rep, head_dim)\n\u001b[0;32m     14\u001b[0m )\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Tensor' object has no attribute 'expands'"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self,args:ModelConfig):\n",
    "        super().__init__()\n",
    "        #根据是否指定n_kv_heads,确定用于键和值的头的数量\n",
    "        self.n_kv_heads  =args.n_heads if args.n_kv_heads is None else args.n_kv_heads\n",
    "        #确保总头数可以被键值头数整除\n",
    "        assert args.n_heads % self.n_kv_heads == 0\n",
    "\n",
    "        #模型并行处理大小，默认为1\n",
    "        model_parallel_size = 1\n",
    "        #本地计算头数，等于总头数除以模型并行处理大小\n",
    "        self.n_local_heads  =args.n_heads //model_parallel_size\n",
    "        #本地键值头数，等于键值头数除以模型并行处理大小\n",
    "        self.n_local_kv_heads = self.n_kv_heads //model_parallel_size\n",
    "        #重复次数，用于扩展键和值的尺寸\n",
    "        self.n_rep  =self.n_local_heads //self.n_local_kv_heads\n",
    "        #每个头的维度，等于模型维度除以头的总数\n",
    "        self.head_dim  =args.dim // args.n_heads\n",
    "\n",
    "        # 定义权重矩阵\n",
    "        self.wq  =nn.Linear(args.dim, args.n_heads * self.head_dim, bias=False)\n",
    "        self.wk = nn.Linear(args.dim, self.n_kv_heads * self.head_dim,bias=False)\n",
    "        self.wv = nn.Linear(args.dim, self.n_kv_heads * self.head_dim,bias=False)\n",
    "\n",
    "        #输出权重矩阵\n",
    "        self.wo = nn.Linear(args.n_heads * self.head_dim,args.dim,bias=False)\n",
    "\n",
    "        #定义dropout\n",
    "        self.attn_dropout  =nn.Dropout(args.dropout)\n",
    "        self.resid_droput = nn.Dropout(args.dropout)\n",
    "\n",
    "        #保存dropout概率\n",
    "        self.dropout = args.dropout\n",
    "\n",
    "        #检查是否使用Flash Attenion(需要Pytorch >=2.0)\n",
    "        self.flash  =hasattr(torch.nn.functional,'scaled_dot_product_attention')\n",
    "\n",
    "        if not self.flash:\n",
    "            #若不支持Flash Attention,则使用手动实现的注意力机制，并设置mask\n",
    "            print(\"Warning: using slow attention. Flash Attention requires PyTorch >= 2.0\")\n",
    "            #创建一个上三角矩阵，用于遮蔽未来信息\n",
    "            mask = torch.full((1,1,args.max_seq_len,args.max_seq_len),float(\"-inf\"))\n",
    "            mask= torch.triu(mask,diagonal=1)\n",
    "\n",
    "            # 注册为模型的缓冲区\n",
    "            self.register_buffer(\"mask\",mask)\n",
    "\n",
    "    def forward(self,x:torch.Tensor, freqs_cos:torch.Tensor,freqs_sin:torch.Tensor):\n",
    "        # 获取批次大小和序列长度，【batch_size,seq_len,dim]\n",
    "        bsz,seqlen,_ = x.shape\n",
    "\n",
    "        #计算Q\\K\\V\n",
    "        xq,xk,xv = self.wq(x),self.wk(x),self.wv(x)\n",
    "\n",
    "        #调整形状以适应头的维度\n",
    "        xq = xq.view(bsz,seqlen,self.n_local_heads,self.head_dim)\n",
    "        xk = xk.view(bsz,seqlen,self.n_local_kv_heads,self.head_dim)\n",
    "        xv = xv.view(bsz,seqlen,self.n_local_kv_heads,self.head_dim)\n",
    "\n",
    "        #应用旋转位置编码\n",
    "        xq,xk =apply_rotary_emb(xq,xk,freqs_cos,freqs_sin)\n",
    "\n",
    "        #对键和值进行扩展以适应重复次数\n",
    "        xk  =repeat_kv(xk,self.n_rep)\n",
    "        xv  =repeat_kv(xv,self.n_rep)\n",
    "\n",
    "        xq = xq.transpose(1,2)\n",
    "        xk = xk.transpose(1,2)\n",
    "        xv = xv.transpose(1,2)\n",
    "\n",
    "        if self.flash:\n",
    "            output = torch.nn.functional.scaled_dot_product_attention(\n",
    "                xq,\n",
    "                xk,\n",
    "                xv,\n",
    "                attn_mask=None,\n",
    "                dropout_p=self.dropout if self.training else 0.0,\n",
    "                is_causal=True\n",
    "            )\n",
    "        else:\n",
    "            #使用手动实现的注意力机制\n",
    "            scores = torch.matmul(xq,xk.transpose(2,3)) / math.sqrt(self.head_dim)\n",
    "            assert hasattr(self,\"mask\")\n",
    "            scores =scores + self.mask[:,:,:seqlen,:seqlen]\n",
    "            scores = F.softmax(scores.float(), dim = -1).type_as(xq)\n",
    "            scores = self.attn_dropout(scores)\n",
    "            output = torch.matmul(scores,xv)\n",
    "\n",
    "        output  =output.transpose(1,2).contiguous().view(bsz,seqlen,-1)\n",
    "\n",
    "        output = self.wo(output)\n",
    "        output  =self.resid_droput(output)\n",
    "        return output\n",
    "\n",
    "\n",
    "attention_model = Attention(args)\n",
    "batch_size =1\n",
    "seq_len = 50\n",
    "dim  =args.dim\n",
    "x = torch.rand(batch_size,seq_len,dim) #随机生成输入张量\n",
    "\n",
    "freqs_cos,freqs_sin = precompute_freqs_cis(dim // args.n_heads, seq_len)\n",
    "output  =attention_model(x, freqs_cos,freqs_sin)\n",
    "\n",
    "print(\"Output shape:\",output.shape)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
